---
title: "Project 1 Summary"
author: "Cecilia Gonzales"
date: "2025-07-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Project Contributors
**Ian McElveen**: 

**Cecilia Gonzales**: Cecilia contributed to this project by doing most of the analysis for questions 1, 2, and 4. This included doing the data cleaning, exploratory analysis, model building, and interpretation of results. She also wrote most of the project summary and helped make the presentation. 

## Project Overview

### Question 1: Are changes in RSI related to team game performance?
  To address whether or not RSI is related to team game performance, our approach took on a similar style to that of previous research. Past research found that RSI was related to explosive in game performance and used top in-game running speed of male D1 Basketball players as their metric. They found that an increase in RSI from the beginning to the end of practice the day before was associated with an in-game top running speed that was greater than their median in-game top running speed for the season. Since our data sets did not have in-game top running speed, a combination of other explosiveness metrics(rebounds, steals, and assists) were used as a proxy to top in-game speed. A game was considered "good" or above median if all three of those metrics were above their season median in a game. On the other hand, a game was considered "bad" or below median if all three of those statistics were below their season median in a game. These thresholds were purposefully made hard to achieve to try to limit variability and concretely distinguish between games that were above and below the season median.
  Previous studies although, was able to measure each player's RSI before and immediately after practice the day before each game. Our data was not complete with RSI measurements before and after each practice so in order to see how RSI changed for players and for the team, changes in RSI were calculated to be the change since the last measurement taken. 
  This analysis built two models, one to predict whether the game would be above median in all three metrics or not and another to predict whether the game would be below all three metrics or not, both using the change in RSI before the game as the only predictor. The models were built the exact same way: a logistic regression model where the binary response was whether or not the game was above median for the first model or below median for the second model. 
  The model that tried to predict whether or not it would be a game with all three metrics above median, it initially found that both the calculated intercept and coefficient associated with the change in RSI were statistically significant at the $\alpha = 0.001$ significance level. This in itself is evidence that changes in RSI are related to in game performance. This model had a cross validated CER of 0.3977 meaning that it will make in incorrect prediction around 39.77% of the time when given new data. The true use of the model was to see if changes in RSI would be able to predict if a game would be considered above median in all metrics so we would like to see how well it actually identified games that were considered above median. For this, we looked at the sensitivity which is how well it identified games that were considered above median. This model had a sensitivity of 0.2358 meaning that given a game was above median in all 3 metrics, the model would predict that it would be above median about 23.58% of the time. This suggests that under these conditions, changes in RSI may not be sensitive enough to fully identify games that would be considered "good" in this analysis.
  The model that tried to predict whether or not it would be a game with all three metrics below median, the intercept and the coefficient associated with the change in RSI as a predictor for whether or not the game would be below median were both statistically significant at the $\alpha = 0.001$ significance level. This suggests that, like for the first model, changes in RSI may have some predictive power for games that were below median in rebounds, steals, and assists. The model had a cross validated CER of 0.2046 meaning that the model would make an incorrect prediction around 20.46% of the time. But, just like before, the main metric that gives insight into whether or not RSI can predict in game performance is the sensitivity of the model. In this case it is 100% meaning that given a game was considered below median in all three metrics, the model will be able to identify that that game will be below median by using change in RSI as the only predictor almost perfectly. 
  The main caveat of this model is that the threshold for games to be considered a success or below median was a probability of 0.05 or roughly 5%. This means that when the model produced probabilities for each game to be a success, the most optimal threshold would be to consider all predictions above 0.05 as a game that would be below median. This is means that changes in RSI don't make "strong" predictions for games that would be below median but it is able to distinctly identify games that are below median and those that are not. The false discovery rate is the rate at which the model will be wrong given that it makes a prediction that an observation is positive. In this case, it is 0.825 meaning that the model will be wrong around 82.5% of the time considering that it makes a positive prediction. This means that with this lower threshold to be considered a success or a below median game with this model, it is over predicting the prevalence of below median games in the data.
  This could be due to a lot of things such as confounding or the fact that for a game to be considered "bad", it had to be below median in all three metrics. The model may also be identifying games in which the team may have performed poorly in two of the metrics but did not meet the threshold to be considered "bad" in this analysis.

### Question 2: Are changes in RSI related to individual statistic game performance?
  A similar approach was taken to answer the question of how changes in RSI are related in individual in-game performance. The same metrics were used to determine whether a game was considered above or below median but it was instead calculated for each individual player and their median statistic values instead of the team as a whole for each game throughout the season. For this analysis, four models were made. The first two were pooled models that tried to predict whether a game was a good game or not or a bad game or not for a player, treating all of the players the same. The last two models were unpooled models and tried to predict whether a game was good or not or bad or not by treating all of the players differently and giving all of them unique slope and intercept values for their logistic regression models. 

#### Pooled Models
  The first model built used a pooled method and tried to predict whether or not a game would be above all three metrics for a player or not using their change in RSI as the only predictor. This means that the model treated all of the players the same and gave them all the same coefficient and intercept value. This model did not find either the intercept nor the coefficient associated with change in RSI to be statistically significant. This model also had a cross validated CER of 0.4685 meaning that it will make an incorrect prediction around 46.85% of the time when looking at new data. This model though has a sensitivity of 0.7964 meaning that it will correctly identify games that are above median in all three statistics around 80% of the time.
  Where most of the error in this model comes from is the false discovery rate. Given that this model makes a prediction that a game will be above median, it will be wrong around 45.93% of the time. This means that the model could be identifying games that are are potentially above median in 2 metrics but were not considered "good" in this analysis due to the high threshold set in the beginning. 
  For the second model, trying to predict whether a game would be below median in all three metrics or not, it was built using a pooled method where all of the players were given the same intercept and slope coefficients. This model found that only the intercept was statistically significant, not the slope coefficient associated with change in RSI. It had a cross validated CER of 0.2046 meaning that it would make an incorrect prediction around 20.46% of the time. This model, similar to the model in the first question has a lower threshold to be considered a success. The threshold to be classified as a success for this model is 0.03. This suggests that change in RSI, when used the exact same way for all players, does not have enough predictive power for to determine whether or not a game will be considered below median. This is supported through the sensitivity and false discovery rates of the model. The sensitivity is 0.2354 meaning that it will correctly identify a game that is considered below median only 23.54% of the time. With this, given that the model predicts a game will be below median, the model is wrong around 87.88% of the time. This suggests that changes in RSI when used the same way for every athlete is a very poor predictor for in game performance. 
  
#### Unpooled Models
  In order to account for the fact that RSI may be a better predictor for in-game success than others, an unpooled model was created for both above and below median games. With these models, we hoped to see if RSI was a better predictor for in-game success for certain players and identify who those players were. 
  For the first model, it was built to predict whether or not a game would be above median in all three metrics for a player or not. This model gave each player their own slope and intercept value. This model had a cross validated error of around 0.4627. But looking at specifically how the model was wrong, the model was stronger than that of the pooled model built before. This model has a sensitivity of 0.7054 suggesting that it correctly identifies games that are above median around 71% of the time. This model also has a false discovery rate of 0.2867 meaning that given the model predicts that a game will be considered above median, it will be wrong about 29% of the time. This improvement from the pooled model suggests that RSI has different impacts on the players individually. Looking deeper into the model, RSI had different predictive power for different athletes. 
  For the unpooled model used to predict whether a game would be below median or not, the cross validated CER was 0.0382. While this is low, it's more important to note that the sensitivity is 0.6471. This means that it will correctly identify games that are considered below median as being below median around 64.71% of the time. With this, given that the model predicts a game will be below median, it will be incorrect around 83.58% of the time. This model outperforms the pooled model in every metric. This further suggests that changes in RSI have different relationships with different athlete's in-game performance. 

##### Different Athlete Impacts
  For games that are considered above median, the athletes that had the most impact when separated from the other athletes were "ID_42", "ID_62", "ID_66". For these three players, RSI was considered to be more impactful of a predictor when it comes to predicting whether or not they will have a below median game. 
  For games that were considered below median, the athletes that had changes in RSI having the most impact were "ID_40" and "ID_42". 

### Question 3: Is the previous week's load related to RSI?


### Question 4: What is each athlete's variation in RSI? What is a meaningful change in RSI for the team, and for the athletes? 
  In order to quantify when RSI measurements were "good" or "bad", they were compared to their mean and how many standard deviations away from that mean that that measurement was. All of the player's RSI measurements were checked and found to have no significant correlation with time suggesting that no one's RSI measurement had a clear increase or decrease with time. The same was checked for the team's averaged RSI data and the same thing was found. This check insured that we weren't missing any significant pattern in the data that may have been useful. 
  When looking at individual player's RSI measurements, we find that there is not a clear trend for lows and highs for all players. In other words, there were no clear points in the season in which all of the players had noticeably high or low RSI measurements. That being said, when looking at players individually, there tends to be a pattern where after a measurement that is at least 1 standard deviation below the player's mean RSI, there tended to be at least one more measurement that was below 1 standard deviation following in the next 1 to 3 measurements. This suggests that while RSI did not clearly have the same pattern for all of the players, we can see that once a player dips somewhat below their normal RSI range, they tend to have another low RSI measurement in the following days to week. The same finding is true for measurements that were at least 1 standard deviation above their mean. This suggests that when a player dips or has a high RSI measurement, then they tend to stay that way for a couple of days to a week.
  Another thing to note from the plots for individual player's RSI is that even though all of the players do not have a linear trend in their data (not monotonically increasing or decreasing with time), players tend to have very different distributions in their RSI measurements. When plotted on the same y-axis, it's clear to see that some players have no overlap with each other in their RSI measurements for the season. While it might be useful to look at the RSI measurement themselves in future analysis, it might also be useful to instead compare an individual RSI measurement to the player's RSI measurement distribution knowing that some players just have lower RSI measurements than others in general. 
  Something interesting that showed up in the plots was that there were no measurements that were below 1 standard deviation for any of the players after the start of February. Before then, there tended to be a relatively consistent number of measurements that were above and below 1 standard deviation for players. There tended to be about 1 player who was at least 1 standard deviation above their mean on a given day and about 2 players who were at least 1 standard deviation below their mean. But, after the start of February, there are no more players who registered even 1 standard deviation below their mean and we see a sharp increase in the number of players who are at least 1 standard deviation above their mean. After the start of February, about 3-4 players tended to have an RSI at least 1 standard deviation above their mean. We looked for an underlying cause for this shift but nothing of significance was found. 

## Code Implementation
  This project's code was written to run sequentially and runs best when you run the questions in the order they were written. This analysis used all of the tidyverse, readxl, aod, dplyr, ggplot2, lubridate, boot, ROCR, purrr, ggforce, lme4, cv, caret, rsample,yardstick, corrplot, and MASS libraries and used the ACWR-KinexonMBB, KinexonSessionMBB, MBB-StatisticTrackingReport, VALD-DynamoMBB, and VALD-ForceDecksMBB data sets. 
  The first section of code is titled "Data Cleaning" and is where we go through each data set and remove columns that are all filled with NA. After removing irrelevant columns, only columns with important information were chosen for each data set and the date column was changed into a date class so that all were standardized across data sets and could be used for plotting. If needed, once only important columns remained, summary statistics and new columns with new information were calculated for each data set.
  The rest of the analysis is broken down into each question. Each section starts by filtering down whichever data set will be used for that analysis into the relevant dates and players, usually taking out players with not enough data and only using data from the most recent Basketball season. After that, each section has some plots with some preliminary exploratory analysis. 
  For the first two questions, lots of summary statistics needed to be calculated for each date but also for each player at each date. In order to accomplish this, loops and group_by() methods were used to isolate each subgroup and calculate the statistic needed. The first question mostly used group_by() but nested for loops were needed to accomplish the more intricate calculations in the second question. 
  When it came to building and getting a preliminary estimate of the error for the models, all of them were built using a training set and testing set made from the entire data set(75% to 25% split from the data). When it came to assessing the models built for questions 1 and 2, all of them were cross validated either using cv.glm() or cv(). All of the plots made in this analysis were built using the ggplot2 library. 
  
## Final Results Summary
### Question 1: Are changes in RSI related to team game performance?
  Changes in RSI when used to predict whether or not a game will be above or below median or not were not entirely useful. In the way that this analysis was done to limit variability and attempt mimic past research. Changes in RSI do not seem useful when it came to predicting in game success.
  The main caveat of this finding is that in order to limit variability, the threshold to be considered "good" or "bad" was purposefully set to be hard to achieve. Given this, using changes in RSI as the only predictor and knowing that it was not calculated in the same way as past research suggests that the findings in this analysis can't completely be compared to those found in past research.
  It's also important to note that in order to look at the team as a whole, change in average RSI for the team was used. As found in Question 4, players do not have similar changes in RSI throughout the season suggesting that averaging removed potentially important patterns in the data. This suggests that looking at changes in RSI at the player level may be more impactful when looking into in-game success. 
  To continue looking into this question, it would be useful to look at this model but lower the threshold for a game to be considered "good" or "bad" since the threshold in this analysis was so high it meant that the models were not nearly sensitive enough to recognize when all three metrics would be above or below median. 
  
### Question 2: Are changes in RSI  related to individual statistic game performance?
  To see if there was a relationship between changes in RSI and in game performance, the same approach from the previous question was used but it used  individual player RSI values and player median statistics instead of averaged team RSI values and team summary statistics. 
  Two models were created to predict whether or not a game would be above median in all three metrics for each player or not and whether or not a game would be below median in all three metrics for each player or not. These models found that when treating all of the players exactly the same in the model that it did not have strong predictive power and could not reliably identify when games would be above or below median or not.
  Two more models were created to predict that same things. But, this time each individual player was given their own intercept and slope value for the logistic regression model. These models found that when differentiating players in the model, they both had improved predictive power from the models that treated all the players the same(pooled models). This suggests that for identifying both above median and below median games, differentiating players leads to improved accuracy within the models. What this finding suggests is that change in RSI has a different relationship with in game success for each player and accounting for it leads to a better understanding of the underlying relationship between change in RSI and in-game performance. 
  The players that had the greatest difference in slopes and intercepts for the model used to predict if it would be above median in all three metrics were "ID_42", "ID_62", and "ID_66". The players that had the most significant difference in slopes and intercepts for the model used to predict if it would be a below median game in all three metrics were 
"ID_40" and "ID_42". In short, I would mostly only use a jump in RSI as an indicator of above median in-game play for "ID_42", "ID_62", and "ID_66" and would only look at a dip in RSI as a potential indicator of below median in game performance for "ID_40" and "ID_42" but would still take predictions with a grain of salt.
  
### Question 3:


### Question 4: What is each athlete's variation in RSI? What is a meaningful change in RSI for the team, and for the athletes?
  Looking at the relationship that RSI had with time throughout the season, there were no clear patterns in rises or falls in RSI for the players. In order to quantify what was a "good", "bad" or "normal" RSI measurement for each player, each of their RSI measurements were compared to their overall season distribution. A measurement was considered "good" if it was above at least 1 standard deviation from their mean and "bad" if it was below at least 1 standard deviation from their mean and "normal" if it did not deviate far from the mean in either direction.
  Looking at the trends that arise from the players throughout the season, there are no clear rises or dips in RSI for the team throughout the season. In fact, they seem to happen randomly for players throughout the season where the number of players who have a rise or dip in RSI stay consistent for each day measurements were taken. This pattern only continues through the end of January though. Once February starts, none of the players register a "bad" RSI measurement for the rest of the season and we actually see a sharp spike in the number of players who have "good" RSI measurements. Nothing in the data we were given can explain this shift in RSI measurements. 
  One pattern that was found to be consistent for most of the players though was that once a player registers a measurement that is one standard deviation below their season mean, it is usually followed by another 1 or 2 measurements that are also at least 1 standard deviation below their season mean in the following days to week. The same goes for measurements that were above at least 1 standard deviation below the mean. This suggests that when players have either dips or highs in RSI they will stay there for a few days before returning to a more "normal" range with small fluctuations within their measurements. 
  It's also important to note that players tended to have very different RSI distributions, sometimes not overlapping at all. Make sure to compare an RSI measurement to a player's RSI distribution to get an understanding of where they are at in terms of neuromuscular fatigue. 

## Final Touches

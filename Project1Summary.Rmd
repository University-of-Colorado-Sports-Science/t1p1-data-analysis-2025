---
title: "Project 1 Summary"
author: "Cecilia Gonzales"
date: "2025-07-01"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Project Contributors

**Ian McElveen**:

**Cecilia Gonzales**:

## Project Overview

### Question 1: Are changes in RSI related to team game performance?

To address whether or not RSI is related to team game performance, our approach took on a similar style to that of previous research.
Past research found that RSI was related to explosive in game performance and used top in-game running speed of male D1 Basketball players as their metric.
They found that an increase in RSI from the beginning to the end of practice the day before was associated with an in-game top running speed that was greater than their median in-game top running speed for the season.
Since our data sets did not have in-game top running speed, a combination of other explosivity metrics(rebounds, steals, and assists) were used as a proxy to top in-game speed.
A game was considered "good" or above median if all three of those metrics were above their season median in a game.
On the other hand, a game was considered "bad" or below median if all three of those statistics were below their season median in a game.
These thresholds were purposefully made hard to achieve to try to limit variability and concretely distinguish between games that were above and below the season median.
Previous studies although, was able to measure each player's RSI before and immediately after practice the day before each game.
Our data was not complete with RSI measurements before and after each practice so in order to see how RSI changed for players and for the team, changes in RSI were calculated to be the change since the last measurement taken.
This analysis built two models, one to predict whether the game would be above median in all three metrics or not and another to predict whether the game would be below all three metrics or not, both using only the change in RSI before the game as the only predictor.
The models were built the exact same way: a logistic regression model where the binary response was whether or not the game was above median for the first model or below median for the second model.
The model that tried to predict whether or not it would be a game with all three metrics above median, it initially found that both the calculated intercept and coefficient associated with the change in RSI were statistically significant at the $\alpha = 0.001$ significance level.
This in itself is evidence that changes in RSI are related to in game performance.
This model had a cross validated CER of 0.3977 meaning that it will make in incorrect prediction around 39.77% of the time when given new data.
The true use of the model was to see if changes in RSI would be able to predict if a game would be considered above median in all metrics so we would like to see how well it actually identified games that were considered above median.
For this, we looked at the sensitivity which is how well it identified games that were considered above median.
This model had a sensitivity of 0.2358 meaning that given a game was above median in all 3 metrics, the model would predict that it would be above median about 23.58% of the time.
This suggests that under these conditions, changes in RSI may not be sensitive enough to fully identify games that would be considered "good" in this analysis.
The model that tried to predict whether or not it would be a game with all three metrics below median, the intercept and the coefficient associated with the change in RSI as a predictor for whether or not the game would be below median were both statistically significant at the $\alpha = 0.001$ significance level.
This suggests that, like for the first model, changes in RSI may have some predictive power for games that were below median in rebounds, steals, and assists.
The model had a cross validated CER of 0.2046 meaning that the model would make an incorrect prediction around 20.46% of the time.
But, just like before, the main metric that gives insight into whether or not RSI can predict in game performance is the sensitivity of the model.
In this case it is 100% meaning that given a game was considered below median in all three metrics, the model will be able to identify that that game will be below median by using change in RSI as the only predictor almost perfectly.
The main caveat of this model is that the threshold for games to be considered a success or below median was a probability of 0.05 or roughly 5%.
This means that when the model produced probabilities for each game to be a success, the most optimal threshold would be to consider all predictions above 0.05 as a game that would be below median.
This is means that changes in RSI don't make "strong" predictions for games that would be below median but it is able to distinctly identify games that are below median and those that are not.
The false discovery rate is the rate at which the model will be wrong given that it makes a prediction that an observation is positive.
In this case, it is 0.825 meaning that the model will be wrong around 82.5% of the time considering that it makes a positive prediction.
This means that with this lower threshold to be considered a success or a below median game with this model, it is over predicting the prevalence of below median games in the data.
This could be due to a lot of things such as confounding or the fact that for a game to be considered "bad", it had to be below median in all three metrics.
The model may also be identifying games in which the team may have performed poorly in two of the metrics but did not meet the threshold to be considered "bad" in this analysis.

### Question 2: Are changes in RSI related to individual statistic game performance?

A similar approach was taken to answer the question of how changes in RSI are related in individual in-game performance.
The same metrics were used to determined whether a game was considered above or below median but it was instead calculated for each individual player and their median statistic values instead of the team as a whole for each game throughout the season.
For this analysis, four models were made.
The first two were pooled models that tried to predict whether a game was a good game or not or a bad game or not for a player, treating all of the players the same.
The last two models were unpooled models and tried to predict whether a game was good or not or bad or not by treating all of the players differently and giving all of them unique slope and intercept values.

#### Pooled Models

The first model built used a pooled method and tried to predict whether or not a game would be above all three metrics for a player or not using their change in RSI as the only predictor.
This means that the model treated all of the players the same and gave them all the same coefficient and intercept value.
This model did not find either the intercept nor the coefficient associated with change in RSI to be statistically significant.
This model also had a cross validated CER of 0.4685 meaning that it will make an incorrect prediction around 46.85% of the time when looking at new data.
This model though has a sensitivity of 0.7964 meaning that it will correctly identify games that are above median in all three statistics around 80% of the time.
Where most of the error in this model comes from is the false discovery rate.
Given that this model makes a prediction that a game will be above median, it will be wrong around 45.93% of the time.
This means that the model could be identifying games that are are potentially above median in 2 metrics but were not considered "good" in this analysis due to the high threshold set in the beginning.
For the second model, it was built using a pooled method where all of the players were given the same intercept and slope coefficients.
This model found that only the intercept was statistically significant, not the slope coefficient associated with change in RSI.
It had a cross validated CER of 0.2046 meaning that it would make an incorrect prediction around 20.46% of the time.
The sensitivity of the model is 0.8826 meaning that it will correctly identify when a player has a game below median around 88.26% of the time.
This model, similar to the model in the first question has a lower threshold to be considered a success.
The threshold to be classified as a success for this model is 0.03.
This suggests that change in RSI, when used the exact same way for all players, does not have enough predictive power for to determine whether or not a game will be considered below median.
This is supported through the sensitivity and false discovery rates of the model.
The sensitivity is 0.2354 meaning that it will correctly identify a game that is considered below median only 23.54% of the time.
With this, given that the model predicts a game will be below median, the model is wrong around 87.88% of the time.
This suggests that changes in RSI when used the same way for every athlete is a very poor predictor for in game performance.

#### Unpooled Models

In order to account for the fact that RSI may be a better predictor for in-game success than others, an unpooled model was created for both above and below median games.
With these models, we hoped to see if RSI was a better predictor for in-game success for certain players and identify who those players were.
For the first model, it was built to predict whether or not a game would be above median in all three metrics for a player or not.
This model gave each player their own slope and intercept value.
This model had a cross validated error of around 0.4627.
But looking at specifically how the model was wrong, the model was stronger than that of the pooled model built before.
This model has a sensitivity of 0.7054 suggesting that it correctly identifies games games that are above median around 71% of the time.
This model also has a false discovery rate of 0.2867 meaning that given the model predicts that a game will be considered above median, it will be wrong about 29% of the time.
This improvement from the pooled model suggests that RSI has different impacts on the players individually.
Looking deeper into the model, RSI had different predictive power for different athletes.
For the unpooled model used to predict whether a game would be below median or not, the cross validated CER was 0.0382.
While this is low, it's more important to note that the sensitivity is 0.6471.
This means that it will correctly identify games that are considered below median as being below median around 64.71% of the time.
With this, given that the model predicts a game will be below median, it will be incorrect around 83.58% of the time.
This model outperforms the pooled model in every metric.
This further suggests that changes in RSI have different relationships with different athlete's in-game performance.

##### Different Athlete Impacts

For games that are considered above median, the athletes that had the most impact when separated from the other athletes were "ID_42", "ID_62", "ID_66".
For these three players, RSI was considered to be more impactful of a predictor when it comes to predicting whether or not they will have a below median game.
For games that were considered below median, the athletes that had changes in RSI having the most impact were "ID_40" and "ID_42".

### Question 3: Is the previous week's load related to RSI?

To investigate whether previous load is related to the Reactive Strength Index, we engineered a set of summary load variables across three distinct time windows preceding each RSI measurement: 7 days, 3 days, and 2 days.
The original dataset contained daily measurements of two key load metrics: **jump load** and **acceleration load**.
For each time window, we created two types of summary variables for both load metrics:

1.  **Average Load**: The mean of daily values across the specified number of days preceding each RSI test.

2.  **Total Load**: The sum of daily values across the same period.

This process resulted in the following 12 new predictor variables: 7-day average jump load, 3-day average jump load, 2-day average jump load, 7-day total jump load, 3-day total jump load, 2-day total jump load, 7-day average acceleration load, 3-day average acceleration load, 2-day average acceleration load, 7-day total acceleration load, 3-day total acceleration load, 2-day total acceleration load.

#### Correlation with RSI

Among the derived variables, the jump load metrics showed the strongest associations with RSI.
The variables most correlated to RSI were:

-   7-day average jump load (TD-6..Avg..Jump.Load): r = 0.26.

-   7-day total jump load (TD-6..Total..Jump.Load): r = 0.24.

-   3-day average jump load (TD-2..Avg..Jump.Load): r = 0.24.

-   2-day average jump load (TD-1..Avg..Jump.Load): r = 0.21.

We also noticed that several of the derived variables were highly correlated with one another, indicating potential multicollinearity issues:

-   **TD-6..Total..Accel.Load** and **TD-6..Total..Jump.Load** were strongly correlated (r = 0.86), yet their associations with RSI differed substantially (r = 0.04 vs. 0.24, respectively).
    Given the weaker association with RSI, TD-6..Total..Accel.Load was **excluded** from modeling.

-   **TD-2..Total..Accel.Load** and **TD-2..Total..Jump.Load** also showed high correlation (r = 0.85), with respective correlations to RSI of -0.01 and 0.16.
    Given the weaker association with RSI, TD-2..Total..Accel.Load was **excluded** during modeling.

#### Modeling the Relationship Between Load and RSI

To explore the predictive relationship between prior training load and Reactive Strength Index (RSI), we fit a series of linear models using the engineered load variables.
Three sets of models were constructed:

1.  **Average Load Model** (lm_avg): Included all of the average jump and acceleration load variables from 7-, 3-, and 2-day windows.
2.  **Total Load Model** (lm_total): Included only the total jump and acceleration load variables across the same time windows.
3.  **Full Model** (lm_full): Included all average and total load variables, with the exception of TD-6..Total..Accel.Load and TD-2..Total..Accel.Load, which were excluded due to high multicollinearity.

To refine each model, we applied stepwise variable selection using Akaike’s Information Criterion (AIC) in both forward and backward directions.
AIC values were compared across all models, and the model with the lowest AIC was **model_step**, derived from the full model.
This final model included **seven load-based variables**, capturing information from multiple time windows and both load types.

The final model was statistically significant overall (p \< 0.001), with an adjusted R² of 0.177, indicating that approximately 18% of the variance in RSI was explained by the selected predictors.The residual standard error was 0.383.

The final model illustrates a complex but interpretable relationship between prior load and RSI:

-   Longer-term acceleration load (7-day) appears to **negatively** impact RSI, likely reflecting the effects of accumulated neuromuscular fatigue.

-   Jump load, particularly over a 7-day period or as a 2-day total, shows a **positive** relationship with RSI, suggesting its role in enhancing explosive readiness when properly dosed.

-   Short-term loads (2–3 days) show mixed effects: while some acceleration and jump loads improve RSI, others are associated with reductions, likely due to acute fatigue.

#### Identifying Optimal Load Zones for Maximizing RSI

To better understand the relationships between training load and Reactive Strength Index (RSI), we generated scatter plots for each engineered load variable.
These visualizations aimed to reveal any observable trends, non-linear patterns, or potential outliers that could inform model development and interpretation.
For each plot, we used scatterplots with **LOESS** (locally estimated scatterplot smoothing) curves to capture potential non-linear relationships without assuming a strict parametric form.

After visualizing the relationships between RSI and various load metrics, we aimed to identify specific load ranges associated with the highest average RSI scores.
Focusing on average acceleration and jump load for the 2-, 3-, and 7-day windows prior to testing, we grouped the data into bins and calculated the mean RSI within each bin to determine which ranges consistently produced better outcomes.
This revealed the following **optimal load zones for maximizing RSI**:

-   2-Day Avg Acceleration Load: 785–835 (mean RSI: 2.12)

-   3-Day Avg Acceleration Load: 785–835 (mean RSI: 2.12)

-   7-Day Avg Acceleration Load: 815–865 (mean RSI: 2.15)

-   2-Day Avg Jump Load: 41200–45200 (mean RSI: 1.85)

-   3-Day Avg Jump Load: 37600–41600 (mean RSI: 1.98)

-   7-Day Avg Jump Load: 27300–31300 (mean RSI: 1.69)

Notably, the optimal ranges for the acceleration 2- and 3-day windows were identical, and the 7-day window shifted slightly higher.
This consistency suggests the presence of a **reliable zone of effective acceleration load** that supports higher RSI scores across multiple time frames.

In contrast, the jump load data **did not exhibit consistent optimal ranges across time windows**, and the associated mean RSI scores were both lower and more variable.
This lack of stability suggests that jump load may have a more complex or context-dependent effect on RSI.
For this reason, we focus our practical recommendations on acceleration load, which demonstrated clearer, more consistent patterns relevant for load management.

#### Individual Athlete Trends

To explore how the relationship between acceleration load and RSI may vary at the individual level, we generated separate plots for each athlete in the dataset.
For every unique athlete ID, we created scatterplots showing RSI against average acceleration load over our three different time windows.
Each plot included a LOESS smoothing curve to visualize potential non-linear trends specific to that athlete.

Furthermore, we made per-athlete pearson correlation coefficients for each load variable.
The results were visualized using horizontal bar plots, where each bar represents one athlete's correlation coefficient between a given load metric and RSI.

These plots revealed substantial variability in how load metrics relate to RSI across athletes.
While some athletes showed positive or negative correlations, others exhibited little to no relationship, suggesting that group-level trends may not generalize uniformly across individuals.

This analysis highlights the potential value of individualized monitoring and load-response profiling in optimizing athlete RSI, rather than relying solely on team-wide patterns.

### Question 4: What is each athlete's variation in RSI? What is a meaningful change in RSI for the team, and for the athletes.

## Code Implementation

The datasets are located under the data-sets folder of this repository.
You will need to extract them from the data-sets-compressed.zip folder into a new folder called data-sets-uncompressed within the data-sets folder.
Once completed, your directory should look this this:

-   t1p1-data-analysis-2025

    -   data-sets

        -   data-sets-uncompressed

            -   data-sets-compressed

                -   Reactive Strength Index

                    -   ACWR - Kinexon MBB.csv

                    -   Kinexon Session MBB.csv

                    -   MBB - Statistic Tracking Report.csv

                    -   VALD - Dynamo MBB.csv

                    -   VALD - ForceDecks MBB.csv

                    -   VALD Normative Data Report - Basketball.html

Someone looking to replicate this type of analysis should install the following packages: tidyverse, readxl, aod, ggplot2, lubridate, boot, ROCR, purrr, ggforce, lme4, cv, caret, rsample, yardstick, knitr, corrplot, MASS, and dplyr.

\

##### Question 3:.

Filter VALD_ForceDecks_clean dataset so that it only contains data from current season (we did it started in October).
When doing this, also only select relevant columns for this question (anon_id, Date, RSI..m.per.s., and Trial).
Sort the data by athlete and then by date.
Group the data by athlete and date.
Add column Daily.Avg.RSI..m.per.s that calculates the daily average RSI for each athlete on each test date.
Ungroup.

Create load dataframe that filters the kinexon_session_clean dataset to only have data from the current season starting in October.
Select relevant columns (anon_id, Date, Daily.Total.Accel.Load.Accum, Jump.Load).
Filter out NA values from Daily.Total.Accel.Load.Accum.

###### Calculate 7, 3, and 2 day load summaries

## Final Results Summary

## Final Touches

---
title: "Project 1 Summary"
author: "Cecilia Gonzales"
date: "2025-07-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Project Contributors
**Ian McElveen**: 

**Cecilia Gonzales**: 

## Project Overview

### Question 1: Are changes in RSI related to team game performance?
  To address whether or not RSI is related to team game performance, our approach took on a similar style to that of previous research. Past research found that RSI was related to explosive in game performance and used top in-game running speed of male D1 Basketball players as their metric. They found that an increase in RSI from the beginning to the end of practice the day before was associated with an in-game top running speed that was greater than their median in-game top running speed for the season. Since our data sets did not have in-game top running speed, a combination of other explosivity metrics(rebounds, steals, and assists) were used as a proxy to top in-game speed. A game was considered "good" or above median if all three of those metrics were above their season median in a game. On the other hand, a game was considered "bad" or below median if all three of those statistics were below their season median in a game. These thresholds were purposefully made hard to achieve to try to limit variability and concretely distinguish between games that were above and below the season median.
  Previous studies although, was able to measure each player's RSI before and immediately after practice the day before each game. Our data was not complete with RSI measurements before and after each practice so in order to see how RSI changed for players and for the team, changes in RSI were calculated to be the change since the last measurement taken. 
  This analysis built two models, one to predict whether the game would be above median in all three metrics or not and another to predict whether the game would be below all three metrics or not, both using only the change in RSI before the game as the only predictor. The models were built the exact same way: a logistic regression model where the binary response was whether or not the game was above median for the first model or below median for the second model. 
  The model that tried to predict whether or not it would be a game with all three metrics above median, it initially found that both the calculated intercept and coefficient associated with the change in RSI were statistically significant at the $\alpha = 0.001$ significance level. This in itself is evidence that changes in RSI are related to in game performance. This model had a cross validated CER of 0.3977 meaning that it will make in incorrect prediction around 39.77% of the time when given new data. The true use of the model was to see if changes in RSI would be able to predict if a game would be considered above median in all metrics so we would like to see how well it actually identified games that were considered above median. For this, we looked at the sensitivity which is how well it identified games that were considered above median. This model had a sensitivity of 0.2358 meaning that given a game was above median in all 3 metrics, the model would predict that it would be above median about 23.58% of the time. This suggests that under these conditions, changes in RSI may not be sensitive enough to fully identify games that would be considered "good" in this analysis.
  The model that tried to predict whether or not it would be a game with all three metrics below median, the intercept and the coefficient associated with the change in RSI as a predictor for whether or not the game would be below median were both statistically significant at the $\alpha = 0.001$ significance level. This suggests that, like for the first model, changes in RSI may have some predictive power for games that were below median in rebounds, steals, and assists. The model had a cross validated CER of 0.2046 meaning that the model would make an incorrect prediction around 20.46% of the time. But, just like before, the main metric that gives insight into whether or not RSI can predict in game performance is the sensitivity of the model. In this case it is 100% meaning that given a game was considered below median in all three metrics, the model will be able to identify that that game will be below median by using change in RSI as the only predictor almost perfectly. 
  The main caveat of this model is that the threshold for games to be considered a success or below median was a probability of 0.05 or roughly 5%. This means that when the model produced probabilities for each game to be a success, the most optimal threshold would be to consider all predictions above 0.05 as a game that would be below median. This is means that changes in RSI don't make "strong" predictions for games that would be below median but it is able to distinctly identify games that are below median and those that are not. The false discovery rate is the rate at which the model will be wrong given that it makes a prediction that an observation is positive. In this case, it is 0.825 meaning that the model will be wrong around 82.5% of the time considering that it makes a positive prediction. This means that with this lower threshold to be considered a success or a below median game with this model, it is over predicting the prevalence of below median games in the data.
  This could be due to a lot of things such as confounding or the fact that for a game to be considered "bad", it had to be below median in all three metrics. The model may also be identifying games in which the team may have performed poorly in two of the metrics but did not meet the threshold to be considered "bad" in this analysis.

### Question 2: Are changes in RSI related to individual statistic game performance?
  A similar approach was taken to answer the question of how changes in RSI are related in individual in-game performance. The same metrics were used to determined whether a game was considered above or below median but it was instead calculated for each individual player and their median statistic values instead of the team as a whole for each game throughout the season. For this analysis, four models were made. The first two were pooled models that tried to predict whether a game was a good game or not or a bad game or not for a player, treating all of the players the same. The last two models were unpooled models and tried to predict whether a game was good or not or bad or not by treating all of the players differently and giving all of them unique slope and intercept values. 

#### Pooled Models
  The first model built used a pooled method and tried to predict whether or not a game would be above all three metrics for a player or not using their change in RSI as the only predictor. This means that the model treated all of the players the same and gave them all the same coefficient and intercept value. This model did not find either the intercept nor the coefficient associated with change in RSI to be statistically significant. This model also had a cross validated CER of 0.4685 meaning that it will make an incorrect prediction around 46.85% of the time when looking at new data. This model though has a sensitivity of 0.7964 meaning that it will correctly identify games that are above median in all three statistics around 80% of the time.
  Where most of the error in this model comes from is the false discovery rate. Given that this model makes a prediction that a game will be above median, it will be wrong around 45.93% of the time. This means that the model could be identifying games that are are potentially above median in 2 metrics but were not considered "good" in this analysis due to the high threshold set in the beginning. 
  For the second model, it was built using a pooled method where all of the players were given the same intercept and slope coefficients. This model found that only the intercept was statistically significant, not the slope coefficient associated with change in RSI. It had a cross validated CER of 0.2046 meaning that it would make an incorrect prediction around 20.46% of the time. The sensitivity of the model is 0.8826 meaning that it will correctly identify when a player has a game below median around 88.26% of the time. This model, similar to the model in the first question has a lower threshold to be considered a success. The threshold to be classified as a success for this model is 0.03. This suggests that change in RSI, when used the exact same way for all players, does not have enough predictive power for to determine whether or not a game will be considered below median. This is supported through the sensitivity and false discovery rates of the model. The sensitivity is 0.2354 meaning that it will correctly identify a game that is considered below median only 23.54% of the time. With this, given that the model predicts a game will be below median, the model is wrong around 87.88% of the time. This suggests that changes in RSI when used the same way for every athlete is a very poor predictor for in game performance. 
  
#### Unpooled Models
  In order to account for the fact that RSI may be a better predictor for in-game success than others, an unpooled model was created for both above and below median games. With these models, we hoped to see if RSI was a better predictor for in-game success for certain players and identify who those players were. 
  For the first model, it was built to predict whether or not a game would be above median in all three metrics for a player or not. This model gave each player their own slope and intercept value. This model had a cross validated error of around 0.4627. But looking at specifically how the model was wrong, the model was stronger than that of the pooled model built before. This model has a sensitivity of 0.7054 suggesting that it correctly identifies games games that are above median around 71% of the time. This model also has a false discovery rate of 0.2867 meaning that given the model predicts that a game will be considered above median, it will be wrong about 29% of the time. This improvement from the pooled model suggests that RSI has different impacts on the players individually. Looking deeper into the model, RSI had different predictive power for different athletes. 
  For the unpooled model used to predict whether a game would be below median or not, the cross validated CER was 0.0382. While this is low, it's more important to note that the sensitivity is 0.6471. This means that it will correctly identify games that are considered below median as being below median around 64.71% of the time. With this, given that the model predicts a game will be below median, it will be incorrect around 83.58% of the time. This model outperforms the pooled model in every metric. This further suggests that changes in RSI have different relationships with different athlete's in-game performance. 

##### Different Athlete Impacts
  For games that are considered above median, the athletes that had the most impact when separated from the other athletes were "ID_42", "ID_62", "ID_66". For these three players, RSI was considered to be more impactful of a predictor when it comes to predicting whether or not they will have a below median game. 
  For games that were considered below median, the athletes that had changes in RSI having the most impact were "ID_40" and "ID_42". 

### Question 3: Is the previous week's load related to RSI?


### Question 4: What is each athlete's variation in RSI? What is a meaningful change in RSI for the team, and for the athletes. 


## Code Implementation


## Final Results Summary


## Final Touches